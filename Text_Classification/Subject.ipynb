{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt`\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding,Flatten,Dense,GlobalAveragePooling1D,GlobalMaxPooling1D,Bidirectional, LSTM,Conv1D\n",
    "from keras.models import Sequential\n",
    "import re\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.optimizers import RMSprop,sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we develop the theory of three-dimensional slo...</td>\n",
       "      <td>Physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>direction of arrival (doa) approximation of ta...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>let $f$ and $g$ be $1$-bounded multiplicative ...</td>\n",
       "      <td>Mathematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we consider the multidimentional brownian cont...</td>\n",
       "      <td>Mathematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inside this paper, the general binary-input bi...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ABSTRACT             class\n",
       "0  we develop the theory of three-dimensional slo...           Physics\n",
       "1  direction of arrival (doa) approximation of ta...        Statistics\n",
       "2  let $f$ and $g$ be $1$-bounded multiplicative ...       Mathematics\n",
       "3  we consider the multidimentional brownian cont...       Mathematics\n",
       "4  inside this paper, the general binary-input bi...  Computer Science"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = TweetTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['ABSTRACT'][:6400]\n",
    "X_val = train['ABSTRACT'][6400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train['class'][:6400]\n",
    "Y_val = train['class'][6400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n"
     ]
    }
   ],
   "source": [
    "X = X_train.to_numpy()\n",
    "for i in range(X.shape[0]):\n",
    "    X[i] = X[i].lower()\n",
    "X = list(X)\n",
    "for i in range(len(X)):\n",
    "    X[i] = re.sub(r'^RT[\\s]+', '', X[i])\n",
    "    X[i] = re.sub(r'https?:\\/\\/.*[\\r\\n]*$-', '', X[i])\n",
    "    X[i] = re.sub(r'#', '', X[i])\n",
    "for i in range(len(X)):\n",
    "    X[i] = token.tokenize(X[i])\n",
    "X_clean = []\n",
    "for i in range(len(X)):\n",
    "    new = []\n",
    "    for word in X[i]:\n",
    "        if(word not in stopwords_english and word not in string.punctuation):\n",
    "            new.append(word)\n",
    "    X_clean.append(new)\n",
    "    #if(i%100 == 0):\n",
    "        #print(i)\n",
    "for i in range(len(X_clean)):\n",
    "    for j in range(len(X_clean[i])):\n",
    "        X_clean[i][j] = stemmer.stem(X_clean[i][j])\n",
    "    if(i%100 == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tok = '<OOV>'\n",
    "trunc_type = 'post'\n",
    "embedding_dim = 32\n",
    "max_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2 = Tokenizer(oov_token = oov_tok )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30139"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.fit_on_texts(X_clean)\n",
    "vocab_size = len(token2.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = token2.texts_to_sequences(X_clean)\n",
    "padded = pad_sequences(sequence,maxlen=max_length,truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_S():\n",
    "    model = Sequential([\n",
    "    Embedding(vocab_size+1,embedding_dim,input_length = max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(32,activation = 'relu'),\n",
    "    Dense(4,activation = 'softmax')])\n",
    "    model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = le.transform(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, ..., 0, 1, 2])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Science', 'Mathematics', 'Physics', 'Statistics']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "Y_onehot = to_categorical(Y,num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modal = Model_S()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHALIN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6400/6400 [==============================] - 5s 802us/step - loss: 1.3003 - accuracy: 0.5075\n",
      "Epoch 2/20\n",
      "6400/6400 [==============================] - 5s 794us/step - loss: 0.9151 - accuracy: 0.6636\n",
      "Epoch 3/20\n",
      "6400/6400 [==============================] - 5s 784us/step - loss: 0.6319 - accuracy: 0.7766\n",
      "Epoch 4/20\n",
      "6400/6400 [==============================] - 5s 791us/step - loss: 0.5135 - accuracy: 0.7928\n",
      "Epoch 5/20\n",
      "6400/6400 [==============================] - 5s 785us/step - loss: 0.4453 - accuracy: 0.8067\n",
      "Epoch 6/20\n",
      "6400/6400 [==============================] - 6s 864us/step - loss: 0.3895 - accuracy: 0.8372\n",
      "Epoch 7/20\n",
      "6400/6400 [==============================] - 5s 772us/step - loss: 0.3297 - accuracy: 0.8745\n",
      "Epoch 8/20\n",
      "6400/6400 [==============================] - 5s 773us/step - loss: 0.2771 - accuracy: 0.9008\n",
      "Epoch 9/20\n",
      "6400/6400 [==============================] - 5s 859us/step - loss: 0.2347 - accuracy: 0.9123\n",
      "Epoch 10/20\n",
      "6400/6400 [==============================] - 5s 790us/step - loss: 0.2022 - accuracy: 0.9275\n",
      "Epoch 11/20\n",
      "6400/6400 [==============================] - 5s 790us/step - loss: 0.1753 - accuracy: 0.9394\n",
      "Epoch 12/20\n",
      "6400/6400 [==============================] - 5s 809us/step - loss: 0.1532 - accuracy: 0.9498\n",
      "Epoch 13/20\n",
      "6400/6400 [==============================] - 5s 792us/step - loss: 0.1312 - accuracy: 0.9588\n",
      "Epoch 14/20\n",
      "6400/6400 [==============================] - 5s 792us/step - loss: 0.1130 - accuracy: 0.9667\n",
      "Epoch 15/20\n",
      "6400/6400 [==============================] - 5s 789us/step - loss: 0.0975 - accuracy: 0.9712\n",
      "Epoch 16/20\n",
      "6400/6400 [==============================] - 5s 779us/step - loss: 0.0825 - accuracy: 0.9772\n",
      "Epoch 17/20\n",
      "6400/6400 [==============================] - 5s 795us/step - loss: 0.0703 - accuracy: 0.9836\n",
      "Epoch 18/20\n",
      "6400/6400 [==============================] - 5s 780us/step - loss: 0.0606 - accuracy: 0.9855\n",
      "Epoch 19/20\n",
      "6400/6400 [==============================] - 5s 816us/step - loss: 0.0500 - accuracy: 0.9898\n",
      "Epoch 20/20\n",
      "6400/6400 [==============================] - 5s 800us/step - loss: 0.0430 - accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19e25fa1240>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Modal.fit(padded,Y_onehot,epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling the Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_classification.pkl']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib    \n",
    "joblib.dump(Modal, 'text_classification.pkl') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving Predictions for the val set using the Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "X_test = X_val.to_numpy()\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = X_test[i].lower()\n",
    "X_test = list(X_test)\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = re.sub(r'^RT[\\s]+', '', X_test[i])\n",
    "    X_test[i] = re.sub(r'https?:\\/\\/.*[\\r\\n]*$-', '', X_test[i])\n",
    "    X_test[i] = re.sub(r'#', '', X_test[i])\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = token.tokenize(X_test[i])\n",
    "X_test_clean = []\n",
    "for i in range(len(X_test)):\n",
    "    new = []\n",
    "    for word in X_test[i]:\n",
    "        if(word not in stopwords_english and word not in string.punctuation):\n",
    "            new.append(word)\n",
    "    X_test_clean.append(new)\n",
    "    #if(i%100 == 0):\n",
    "        #print(i)\n",
    "for i in range(len(X_test_clean)):\n",
    "    for j in range(len(X_test_clean[i])):\n",
    "        X_test_clean[i][j] = stemmer.stem(X_test_clean[i][j])\n",
    "    if(i%100 == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_val = token2.texts_to_sequences(X_test_clean)\n",
    "padded_val = pad_sequences(sequence_val,maxlen=max_length,truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction_temp = Modal.predict(padded_val)\n",
    "Prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1600):\n",
    "    Prediction.append(np.argmax(Prediction_temp[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = le.inverse_transform(Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853125"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_val,Prediction,average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We got an F1-Score of 85.3125 on our Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
